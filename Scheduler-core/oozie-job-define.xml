<workflow-app xmlns="uri:oozie:workflow:0.5" name="workflow-flowName_2018-01-23 15:32:02">
  <start to="fork0"/>
  <fork name="fork0">
    <path start="stateModel1_etl_read_csv"/>
    <path start="stateModel2_etl_read_csv"/>
  </fork>
  <join name="join0" to="stateModel4_predict"/>
  <action name="stateModel1_etl_read_csv">
    <spark xmlns="uri:oozie:spark-action:0.1">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
        <property>
          <name>oozie.service.SparkConfigurationService.spark.configurations</name>
          <value>spark.eventLog.dir=hdfs://172.16.0.132:8020/user/spark/spark2ApplicationHistory,spark.yarn.historyServer.address=http://172.16.0.132:19888,spark.eventLog.enabled=true</value>
        </property>
      </configuration>
      <master>yarn-cluster</master>
      <name>spark scala job</name>
      <class>youe.data.scala.drivers.EtlDriver2</class>
      <jar>youe.scala.2.0-0.0.1-SNAPSHOT.jar</jar>
      <spark-opts>--conf spark.yarn.jars=/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/*</spark-opts>
      <arg>${stateModel1_etl_read_csv_function}</arg>
      <arg>${stateModel1_etl_read_csv_inputPath}</arg>
      <arg>${stateModel1_etl_read_csv_delimiter}</arg>
      <arg>${stateModel1_etl_read_csv_header}</arg>
      <arg>${stateModel1_etl_read_csv_charSet}</arg>
      <arg>${stateModel1_etl_read_csv_outputPath}</arg>
    </spark>
    <ok to="stateModel3_mlp"/>
    <error to="fail"/>
  </action>
  <action name="stateModel3_mlp">
    <spark xmlns="uri:oozie:spark-action:0.1">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
        <property>
          <name>oozie.service.SparkConfigurationService.spark.configurations</name>
          <value>spark.eventLog.dir=hdfs://172.16.0.132:8020/user/spark/spark2ApplicationHistory,spark.yarn.historyServer.address=http://172.16.0.132:19888,spark.eventLog.enabled=true</value>
        </property>
      </configuration>
      <master>yarn-cluster</master>
      <name>spark scala job</name>
      <class>youe.data.scala.drivers.MachineLearnDriver2</class>
      <jar>youe.scala.2.0-0.0.1-SNAPSHOT.jar</jar>
      <spark-opts>--conf spark.yarn.jars=/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/*</spark-opts>
      <arg>${stateModel3_mlp_function}</arg>
      <arg>${stateModel3_mlp_usedFields}</arg>
      <arg>${stateModel3_mlp_labelCol}</arg>
      <arg>${stateModel3_mlp_nodesHide}</arg>
      <arg>${stateModel3_mlp_maxIter}</arg>
      <arg>${stateModel3_mlp_delimiter}</arg>
      <arg>${stateModel3_mlp_outputPath}</arg>
      <arg>${stateModel3_mlp_inputPath}</arg>
    </spark>
    <ok to="join0"/>
    <error to="fail"/>
  </action>
  <action name="stateModel4_predict">
    <spark xmlns="uri:oozie:spark-action:0.1">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
        <property>
          <name>oozie.service.SparkConfigurationService.spark.configurations</name>
          <value>spark.eventLog.dir=hdfs://172.16.0.132:8020/user/spark/spark2ApplicationHistory,spark.yarn.historyServer.address=http://172.16.0.132:19888,spark.eventLog.enabled=true</value>
        </property>
      </configuration>
      <master>yarn-cluster</master>
      <name>spark scala job</name>
      <class>youe.data.scala.drivers.MachineLearnDriver2</class>
      <jar>youe.scala.2.0-0.0.1-SNAPSHOT.jar</jar>
      <spark-opts>--conf spark.yarn.jars=/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/*</spark-opts>
      <arg>${stateModel4_predict_function}</arg>
      <arg>${stateModel4_predict_delimiter}</arg>
      <arg>${stateModel4_predict_outputPath}</arg>
      <arg>${stateModel4_predict_inputPathData}</arg>
      <arg>${stateModel4_predict_inputPathModel}</arg>
      <arg>${stateModel4_predict_header}</arg>
    </spark>
    <ok to="stateModel5_etl_write_csv"/>
    <error to="fail"/>
  </action>
  <action name="stateModel5_etl_write_csv">
    <spark xmlns="uri:oozie:spark-action:0.1">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
        <property>
          <name>oozie.service.SparkConfigurationService.spark.configurations</name>
          <value>spark.eventLog.dir=hdfs://172.16.0.132:8020/user/spark/spark2ApplicationHistory,spark.yarn.historyServer.address=http://172.16.0.132:19888,spark.eventLog.enabled=true</value>
        </property>
      </configuration>
      <master>yarn-cluster</master>
      <name>spark scala job</name>
      <class>youe.data.scala.drivers.EtlDriver2</class>
      <jar>youe.scala.2.0-0.0.1-SNAPSHOT.jar</jar>
      <spark-opts>--conf spark.yarn.jars=/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/*</spark-opts>
      <arg>${stateModel5_etl_write_csv_function}</arg>
      <arg>${stateModel5_etl_write_csv_fileFormat}</arg>
      <arg>${stateModel5_etl_write_csv_delimiter}</arg>
      <arg>${stateModel5_etl_write_csv_outputPath}</arg>
      <arg>${stateModel5_etl_write_csv_charSet}</arg>
      <arg>${stateModel5_etl_write_csv_filename}</arg>
      <arg>${stateModel5_etl_write_csv_inputName}</arg>
      <arg>${stateModel5_etl_write_csv_inputPath}</arg>
    </spark>
    <ok to="end"/>
    <error to="fail"/>
  </action>
  <action name="stateModel2_etl_read_csv">
    <spark xmlns="uri:oozie:spark-action:0.1">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
        <property>
          <name>oozie.service.SparkConfigurationService.spark.configurations</name>
          <value>spark.eventLog.dir=hdfs://172.16.0.132:8020/user/spark/spark2ApplicationHistory,spark.yarn.historyServer.address=http://172.16.0.132:19888,spark.eventLog.enabled=true</value>
        </property>
      </configuration>
      <master>yarn-cluster</master>
      <name>spark scala job</name>
      <class>youe.data.scala.drivers.EtlDriver2</class>
      <jar>youe.scala.2.0-0.0.1-SNAPSHOT.jar</jar>
      <spark-opts>--conf spark.yarn.jars=/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/*</spark-opts>
      <arg>${stateModel2_etl_read_csv_function}</arg>
      <arg>${stateModel2_etl_read_csv_inputPath}</arg>
      <arg>${stateModel2_etl_read_csv_delimiter}</arg>
      <arg>${stateModel2_etl_read_csv_header}</arg>
      <arg>${stateModel2_etl_read_csv_charSet}</arg>
      <arg>${stateModel2_etl_read_csv_outputPath}</arg>
    </spark>
    <ok to="join0"/>
    <error to="fail"/>
  </action>
  <kill name="fail">
    <message>something went wrong : ${wf:errorCode('wordcount')}</message>
  </kill>
  <end name="end"/>
</workflow-app>
